All three works are focused on the relationship between machine learning algorithms and their datasets, specifically the biases that can develop. As they point out, data is collected via integrations with the internet and/or institutions that often restrict access due to socio-economic status. Things like hospitals, computers, etc., are more accessible to the middle class and above of wealthy nations, which is disproportionally dominated by white people, and even more specifically white men. Therefore, these algorithms grow to perpetuate the inequalities that already exist in society, working to further disenfranchise historically disadvantaged groups such as women, people of color, and people of the lgbt+ community to list a few.

The articles were obviously upsetting and disheartening to read. Over the past 70 years or so, we have increasingly given over control of critical and influential systems to computers for their efficiency and perceived neutrality. However, these articles are showing that there exist deep flaws imbedded in our current approach. The evidence that these systems are actually making life worse for some groups of peoples feels incredibly dystopian. The group was fairly united on this too as I don't think there is much of a positive spin to put on this besides that we might be able to fix it given a few decades

As I mentioned in the first paragraph, one of the most difficult elements is that while all people (from a racial and gender-based perspective) generally have similar needs for many institutions, such as health care or the internet, due to mitigating circumstances, their treatment/response can change and thus their perceived needs can be very different from their actual needs. Additionally, we don't currently have great methods to measure these unconsidered biases in datasets as we ourselves aren't necessarily aware of them. Even when we are aware, filtering a large dataset to remove bias is a task that needs ML algorithms, forming a chicken and the egg situation. 

In that machine learning algorithms behave in a predictable manner (assuming we can understand the underlying structure) for a given task, they still could be considered 'unbiased'. However, from a larger perspective they really don't. They learn the underlying socio-economic discrimination occurring in society and perpetuate it. I think that the solution should be approached from two directions. The first is that while the task is difficult now, we should look at methods to find implicit biases in data sets perhaps via clustering algorithms or unsupervised learning. Concurrently with that, the obvious solution is to address the inequalities that lead to the discriminatory sampling in the first place. Not to get to socialist here, but this means stronger social programs and safety nets to allow those who are currently disadvantaged the means to improve their lot. 